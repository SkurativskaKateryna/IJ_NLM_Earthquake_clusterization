{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVSrmQMSdwWo"
      },
      "source": [
        "# LIBRARIES AND DATA LOADS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW6xGcYc0_RA"
      },
      "outputs": [],
      "source": [
        "%pip install contextily\n",
        "%pip install plotly\n",
        "%pip install geopandas\n",
        "%pip install fastkml fiona shapely lxml\n",
        "%pip install hdbscan\n",
        "%pip install --upgrade hdbscan scikit-learn\n",
        "# !pip install hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "619eF8n4Z3s6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import tempfile\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import fiona\n",
        "import requests\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    silhouette_samples,\n",
        "    adjusted_rand_score,\n",
        "    davies_bouldin_score,\n",
        "    calinski_harabasz_score\n",
        ")\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "import hdbscan\n",
        "\n",
        "# Suppress future warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk79JB720_RB"
      },
      "source": [
        "# FUNCTIONS TO PLOT RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCbYL9wZ0_RB"
      },
      "source": [
        "The function to plot points from dataframe and it allows to select the style of the map of plotting:\n",
        "\n",
        "\n",
        "| **Style Name**                  | **Description** |\n",
        "|---------------------------------|---------------------------------------------------------------|\n",
        "| `\"basic\"`                        | Simple, clean map with standard colors and labels. |\n",
        "| `\"carto-darkmatter\"`              | Dark-themed map with high contrast, designed for nighttime use. |\n",
        "| `\"carto-darkmatter-nolabels\"`     | Dark map without labels, useful for minimalistic designs. |\n",
        "| `\"carto-positron\"`                | Light-themed, minimalistic map with soft colors. |\n",
        "| `\"carto-positron-nolabels\"`       | Light-colored map without labels, good for data overlays. |\n",
        "| `\"carto-voyager\"`                 | General-purpose map with a balance of detail and simplicity. |\n",
        "| `\"carto-voyager-nolabels\"`        | Same as Voyager but without labels. |\n",
        "| `\"dark\"`                          | Dark mode map similar to `\"carto-darkmatter\"`, with more vibrant colors. |\n",
        "| `\"light\"`                         | Light-themed map similar to `\"carto-positron\"`, with more contrast. |\n",
        "| `\"open-street-map\"`               | Standard OpenStreetMap tiles, free to use. |\n",
        "| `\"outdoors\"`                      | Outdoor-style map, includes terrain details, parks, and hiking trails. |\n",
        "| `\"satellite\"`                     | Pure satellite imagery without any additional overlays. |\n",
        "| `\"satellite-streets\"`             | Satellite imagery with streets, roads, and place names overlaid. |\n",
        "| `\"streets\"`                       | Standard street map with clear roads and points of interest. |\n",
        "| `\"white-bg\"`                      | Blank white canvas (no map). Useful for custom layers and no external HTTP requests. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rXdCz2LI0_RC"
      },
      "outputs": [],
      "source": [
        "def plot_data(df, faults_df, style=\"carto-positron\"):\n",
        "    # Scale 'z' values to max 15\n",
        "    max_size = 15\n",
        "    z_scaled = df['z'] / df['z'].max() * max_size\n",
        "    z_scaled = z_scaled.clip(1, max_size)  # Optional: prevent too-small points\n",
        "\n",
        "    # Create earthquake scatter map\n",
        "    fig = go.Figure(go.Scattermap(\n",
        "        lon=df['x'], lat=df['y'],\n",
        "        marker=dict(size=z_scaled, color=\"red\", opacity=0.6),\n",
        "        text=df['z'],\n",
        "        name=\"Earthquakes\"\n",
        "    ))\n",
        "\n",
        "    # Add faults\n",
        "    fig.add_trace(go.Scattermap(\n",
        "        lat=faults_df[\"Latitude\"],\n",
        "        lon=faults_df[\"Longitude\"],\n",
        "        mode='markers',\n",
        "        marker=dict(size=2, color='#3776ab'),\n",
        "        name=\"Faults\"\n",
        "    ))\n",
        "\n",
        "    # Map layout\n",
        "    fig.update_layout(\n",
        "        map=dict(\n",
        "            style=style,\n",
        "            center=dict(lon=df['x'].median(), lat=df['y'].median()),\n",
        "            zoom=5.5,\n",
        "        ),\n",
        "        width=950,\n",
        "        height=800,\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9mRzITP0_RC"
      },
      "source": [
        "## GET THE PARAMETERS SET FROM RESULTS_DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vHuw1VQu0_RC"
      },
      "outputs": [],
      "source": [
        "def get_param_key(mode, result_df, line_id):\n",
        "    \"\"\"\n",
        "    Generates a tuple of parameters from a row ID in result_df.\n",
        "\n",
        "    Args:\n",
        "        result_df (pd.DataFrame): DataFrame containing clustering results.\n",
        "        line_id (int): Index of the row to extract parameters from.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Formatted parameter key as a tuple of (key, value) pairs.\n",
        "    \"\"\"\n",
        "    if line_id not in result_df.index:\n",
        "        raise ValueError(f\"Row ID {line_id} not found in DataFrame.\")\n",
        "\n",
        "    if mode == 'kmeans':\n",
        "        row = result_df.loc[line_id, ['init', 'max_iter', 'n_clusters', 'n_init']].to_dict()\n",
        "        param_key = tuple(row.items())\n",
        "\n",
        "    if mode == 'dbscan':\n",
        "        row = result_df.loc[line_id, ['eps',  'metric', 'min_samples']].to_dict()\n",
        "        param_key = tuple(row.items())\n",
        "\n",
        "    if mode == 'hdbscan':\n",
        "        row = result_df.loc[line_id, ['metric', 'min_cluster_size']].to_dict()\n",
        "        param_key = tuple((k, None if pd.isna(v) else v) for k, v in row.items())\n",
        "\n",
        "    return param_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwWFTY5X0_RC"
      },
      "source": [
        "## PLOT CLUSTERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oX_1_yfB0_RC"
      },
      "outputs": [],
      "source": [
        "def plot_data_clusters(results_df, clusters_dict,  faults_df, id, mode, style=\"carto-positron\"):\n",
        "    \"\"\"\n",
        "    Plots clustered data on a map using Plotly (go.Scattermap) with a proper legend.\n",
        "\n",
        "    Args:\n",
        "        clusters_dict (dict): Dictionary containing clustering results.\n",
        "        param_key (tuple): The specific parameter set as a tuple (same format as dictionary keys).\n",
        "        style (str): Map style (default: \"carto-positron\").\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    param_key = get_param_key(mode, results_df, id)\n",
        "    if 'data' not in clusters_dict:\n",
        "        raise KeyError(\"The dictionary must contain an entry with key 'data' containing the original dataset.\")\n",
        "\n",
        "    if param_key not in clusters_dict:\n",
        "        raise KeyError(f\"Parameters {param_key} not found in clusters_dict.\")\n",
        "\n",
        "    # Retrieve original data\n",
        "    data = clusters_dict['data'].copy()\n",
        "\n",
        "    # Retrieve cluster labels\n",
        "    data['cluster'] = clusters_dict[param_key]\n",
        "\n",
        "    # Create a scatter map figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(go.Scattermap(\n",
        "            lat=faults_df[\"Latitude\"],\n",
        "            lon=faults_df[\"Longitude\"],\n",
        "            mode='markers',\n",
        "            marker=go.scattermap.Marker(\n",
        "                size=2, color='#3776ab'\n",
        "            ),\n",
        "            name=\"Faults\"\n",
        "        ))\n",
        "\n",
        "    # Get unique clusters\n",
        "    unique_clusters = sorted(data['cluster'].unique())\n",
        "\n",
        "    # Define a color map for clusters\n",
        "    color_map = px.colors.qualitative.Set1  # Change color scheme if needed\n",
        "\n",
        "    # Plot each cluster separately to show legend\n",
        "    for i, cluster in enumerate(unique_clusters):\n",
        "        cluster_data = data[data['cluster'] == cluster]\n",
        "\n",
        "        fig.add_trace(go.Scattermap(\n",
        "            lon=cluster_data['x'], lat=cluster_data['y'],\n",
        "            mode=\"markers\",\n",
        "            marker=dict(\n",
        "                size=4,\n",
        "                color=color_map[i % len(color_map)],  # Assign color from color_map\n",
        "                opacity=0.7\n",
        "            ),\n",
        "            text=[f\"Cluster {cluster}\"] * len(cluster_data),  # Hover text\n",
        "            name=f\"Cluster {cluster}\"  # Important: This ensures a legend entry\n",
        "        ))\n",
        "\n",
        "    # Format param_key into a more readable title\n",
        "    formatted_params = ', '.join(f\"{key} = {value}\" for key, value in param_key)\n",
        "    # Convert the row into a list of dictionaries, then extract the first one\n",
        "    param_stat_list = results_df.loc[results_df['index'] == id, ['silhouette_score', 'dbi', 'chs']].to_dict(orient='records')\n",
        "    param_stat = param_stat_list[0] if param_stat_list else {}\n",
        "    formatted_stat = ', '.join(f\"{key} = {value:0.3f}\" for key, value in param_stat.items())\n",
        "    # Update layout with map style\n",
        "    fig.update_layout(\n",
        "        map={\n",
        "            'style': style,\n",
        "            'center': {'lon': data['x'].median(), 'lat': data['y'].median()},\n",
        "            'zoom': 5.5\n",
        "        },\n",
        "        width=950,\n",
        "        height=800,\n",
        "        showlegend=True,  # Ensures legend is displayed\n",
        "        title=f\"Cluster Visualization for parameters: {formatted_params} <br> with {formatted_stat}\"\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8nzZaUV0_RD"
      },
      "source": [
        "## PLOT 2-DIM STATS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5oxMNYfd0_RD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_2d_stat(results_df, x_variable, y_variable, color_variable, title=''):\n",
        "\n",
        "    hover_data = [col for col in results_df.columns]\n",
        "\n",
        "    fig = px.scatter(\n",
        "        results_df,\n",
        "        x=x_variable,\n",
        "        y=y_variable,\n",
        "        color=color_variable,\n",
        "        color_continuous_scale='plasma',\n",
        "        size_max=32,\n",
        "        hover_data=hover_data,  # Include the index in hover data\n",
        "        title=title\n",
        "    )\n",
        "    fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpv_KUSs0_RD"
      },
      "source": [
        "## ARI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7BWBo8OP0_RD"
      },
      "outputs": [],
      "source": [
        "def calculate_ari(method, results_df,  cluster_dict, num_clusters ,silhouette_score_level ):\n",
        "  # Calculate Adjusted Rand Index for each pair of label sets\n",
        "  ari_data = {}\n",
        "\n",
        "  labels = results_df[(results_df['silhouette_score']> silhouette_score_level) &(results_df['n_clusters']==num_clusters)]['index'].unique()\n",
        "  for label1 in labels:\n",
        "      ari_data[label1] = {}\n",
        "      key_param_1 = get_param_key(method, results_df, label1)\n",
        "      for label2 in labels:\n",
        "          key_param_2 = get_param_key(method, results_df, label2)\n",
        "          ari_data[label1][label2] = adjusted_rand_score(cluster_dict[key_param_1], cluster_dict[key_param_2])\n",
        "\n",
        "  # Create a DataFrame from the nested dictionary\n",
        "  ari_df = pd.DataFrame(ari_data)\n",
        "  return ari_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_PqEPDqj0_RD"
      },
      "outputs": [],
      "source": [
        "def plot_ari(ari_df):\n",
        "\n",
        "    fig = px.imshow(\n",
        "        ari_df.values,\n",
        "        text_auto='.2f',\n",
        "        color_continuous_scale='RdBu_r',\n",
        "        aspect='auto',\n",
        "        zmin=0,\n",
        "        zmax=1\n",
        "    )\n",
        "\n",
        "    # Custom axis labels\n",
        "    fig.update_xaxes(\n",
        "        tickmode='array',\n",
        "        tickvals=list(range(len(ari_df.columns))),\n",
        "        ticktext=ari_df.columns\n",
        "    )\n",
        "\n",
        "    fig.update_yaxes(\n",
        "        tickmode='array',\n",
        "        tickvals=list(range(len(ari_df.index))),\n",
        "        ticktext=ari_df.index\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"ARI (0 to 1 Scale)\",\n",
        "        xaxis_title=\"\",\n",
        "        yaxis_title=\"\"\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uJBoBrQj0_RD"
      },
      "outputs": [],
      "source": [
        "def calculate_ari_all(method, results_df, clusters_dict, silhouette_score_level = 0.0):\n",
        "\n",
        "    num_clusters = results_df[results_df['silhouette_score']> silhouette_score_level]['n_clusters'].unique()\n",
        "\n",
        "    for num_cluster in num_clusters:\n",
        "        n = results_df[(results_df['n_clusters']==num_cluster) & (results_df['silhouette_score']> silhouette_score_level)]['index'].nunique()\n",
        "        uni = results_df[(results_df['n_clusters']==num_cluster) & (results_df['silhouette_score']> silhouette_score_level)]['index'].unique()\n",
        "        print(f\"{n} ids with {num_cluster} number of clusters: {uni}\")\n",
        "        ari_df = calculate_ari(method, results_df,clusters_dict, num_cluster, silhouette_score_level)\n",
        "        plot_ari(ari_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDEPT5BP0_RE"
      },
      "source": [
        "# DATA LOAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Vu_Fymhg0_RE"
      },
      "outputs": [],
      "source": [
        "def read_github_kmz_faults(github_folder_url, kmz_filenames):\n",
        "    \"\"\"\n",
        "    Reads KMZ files stored in a GitHub repository, extracts KML data, and returns a DataFrame of fault coordinates.\n",
        "\n",
        "    Args:\n",
        "        github_folder_url (str): Base URL of the GitHub repository's folder (without filenames).\n",
        "        kmz_filenames (list): List of KMZ filenames to fetch from GitHub.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing fault coordinates from KMZ files.\n",
        "    \"\"\"\n",
        "\n",
        "    all_fault_points = []  # Store extracted points\n",
        "\n",
        "    # Temporary folder for extracted KML files\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "    for kmz_file in kmz_filenames:\n",
        "        kmz_url = f\"{github_folder_url}/{kmz_file}\"  # Construct the GitHub raw file URL\n",
        "\n",
        "        try:\n",
        "            # Download the KMZ file from GitHub\n",
        "            response = requests.get(kmz_url)\n",
        "            response.raise_for_status()  # Raise error for failed requests\n",
        "\n",
        "            # Extract KMZ in memory\n",
        "            with ZipFile(BytesIO(response.content), 'r') as zip_ref:\n",
        "                zip_ref.extractall(temp_dir)  # Extract to temp directory\n",
        "\n",
        "            # Locate the extracted KML file (most KMZs contain \"doc.kml\")\n",
        "            kml_file = os.path.join(temp_dir, \"doc.kml\")\n",
        "\n",
        "            # Get available layers\n",
        "            layers = fiona.listlayers(kml_file)\n",
        "\n",
        "            # Loop through each layer\n",
        "            for layer in layers:\n",
        "                gdf = gpd.read_file(kml_file, driver=\"KML\", layer=layer)\n",
        "\n",
        "                if gdf.empty:\n",
        "                    continue\n",
        "\n",
        "                gdf = gdf.explode(index_parts=True, ignore_index=True)  # Ensure correct format\n",
        "                gdf[\"coords\"] = gdf[\"geometry\"].apply(lambda geom: list(geom.coords) if geom else None)\n",
        "\n",
        "                # Store extracted points\n",
        "                for _, row in gdf.iterrows():\n",
        "                    if row[\"coords\"]:\n",
        "                        for coord in row[\"coords\"]:\n",
        "                            # Handle both (lon, lat) and (lon, lat, alt) cases\n",
        "                            lon, lat = coord[:2]  # Ignore altitude if present\n",
        "                            all_fault_points.append({\"Longitude\": lon, \"Latitude\": lat, \"File\": kmz_file})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {kmz_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    faults_df = pd.DataFrame(all_fault_points)\n",
        "\n",
        "    return faults_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_1QEoNS0_RE"
      },
      "outputs": [],
      "source": [
        "github_kmz_folder = \"https://raw.githubusercontent.com/SkurativskaKateryna/IJ_NLM_Earthquake_clusterization/refs/heads/main/DATA/\"\n",
        "faults_df = read_github_kmz_faults(github_kmz_folder, kmz_filenames=[\"AFEAD_J38.kmz\",\"AFEAD_J39.kmz\",\"AFEAD_K38.kmz\",\"AFEAD_K39.kmz\"])\n",
        "print(faults_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjFuteNQ0_RE",
        "outputId": "25e12e18-4c48-4b0e-8d3f-db612dbb965c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6196, 4)\n"
          ]
        }
      ],
      "source": [
        "# Define URL and file name\n",
        "base_url = \"https://raw.githubusercontent.com/SkurativskaKateryna/IJ_NLM_Earthquake_clusterization/refs/heads/main/DATA/\"\n",
        "file_name = \"Earthquake_depth6000ClearNew.txt\"\n",
        "path = base_url + file_name\n",
        "\n",
        "data = pd.read_csv(path,delimiter=\"\\t\", header=None, names=[\"y\", \"x\", \"z\"])[[\"x\", \"y\", \"z\"]]\n",
        "data['x'] = pd.to_numeric(data['x'], errors='coerce')\n",
        "data = data.dropna()\n",
        "data = data[data['y'] > 0].reset_index(drop=True)\n",
        "scaler = Normalizer()\n",
        "data['scaled_z'] = scaler.fit_transform(data[['z']])\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOvZJEms0_RE"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwshKIuO0_RE"
      },
      "source": [
        "# DATA OVERVIEW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CUHZhDR0_RF"
      },
      "outputs": [],
      "source": [
        "# Print basic statistics\n",
        "print(\"Shape:\", data.shape)\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(data.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJH_Vype0_RF"
      },
      "outputs": [],
      "source": [
        "plot_data(data, faults_df, style=\"carto-positron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2l8Y6PJ0_RF"
      },
      "source": [
        "# CLUSTERIZATION\n",
        "## KMEAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfZjJ8mV0_RF"
      },
      "outputs": [],
      "source": [
        "def KMEAN_gridsearch(df):\n",
        "    # Define parameter grid\n",
        "    param_grid = {\n",
        "        'n_clusters': list(range(2, int(np.sqrt(len(df))))),  # Range of clusters\n",
        "        'init': ['k-means++', 'random'],  # Initialization methods\n",
        "        'n_init': [ 30],  # Number of random restarts\n",
        "        'max_iter': [300] #, 500]  # Max iterations\n",
        "    }\n",
        "\n",
        "\n",
        "    # Store results\n",
        "    results = []\n",
        "    clusters_dict = {}  # Dictionary to store cluster labels\n",
        "    clusters_dict['data'] = df[['x', 'y', 'z']] # Store original data\n",
        "\n",
        "    # Total number of iterations\n",
        "    total_combinations = len(list(ParameterGrid(param_grid)))\n",
        "    current_iteration = 0\n",
        "\n",
        "    # Grid Search over parameters\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        current_iteration += 1\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Iteration {current_iteration}/{total_combinations}: Testing parameters {params}\")\n",
        "\n",
        "        # Apply KMeans clustering\n",
        "        kmeans = KMeans(\n",
        "            n_clusters=params['n_clusters'],\n",
        "            init=params['init'],\n",
        "            n_init=params['n_init'],\n",
        "            max_iter=params['max_iter'],\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        cluster_labels = kmeans.fit_predict(df[['x', 'y', 'scaled_z']])\n",
        "\n",
        "        # Compute metrics\n",
        "        inertia = kmeans.inertia_\n",
        "        silhouette_avg = silhouette_score(df[['x', 'y', 'scaled_z']], cluster_labels)\n",
        "        dbi = davies_bouldin_score(df[['x', 'y', 'scaled_z']], cluster_labels)\n",
        "        chs = calinski_harabasz_score(df[['x', 'y', 'scaled_z']], cluster_labels)\n",
        "\n",
        "        # Compute silhouette scores for each sample\n",
        "        silhouette_values = silhouette_samples(df[['x', 'y', 'scaled_z']], cluster_labels)\n",
        "        silhouette_per_cluster = {label: [] for label in cluster_labels if label != -1}  # Exclude noise (-1)\n",
        "        for label, score in zip(cluster_labels, silhouette_values):\n",
        "            if label != -1:  # Exclude noise points from per-cluster silhouette scores\n",
        "                silhouette_per_cluster[label].append(score)\n",
        "\n",
        "        # Print results for each iteration\n",
        "        print(f\"  - Silhouette Score: {silhouette_avg:.4f}, Inertia: {inertia:.2f}, DBI: {dbi:.4f}, CHS: {chs:.4f}\")\n",
        "\n",
        "        # Store results in DataFrame format\n",
        "        results.append({\n",
        "            'n_clusters': params['n_clusters'],\n",
        "            'init': params['init'],\n",
        "            'n_init': params['n_init'],\n",
        "            'max_iter': params['max_iter'],\n",
        "            'inertia': inertia,\n",
        "            'silhouette_score': silhouette_avg,\n",
        "            'dbi': dbi,\n",
        "            'chs': chs,\n",
        "            'silhouette_per_cluster': silhouette_per_cluster\n",
        "        })\n",
        "\n",
        "        # Store cluster labels in dictionary (using tuple of params as key)\n",
        "        clusters_dict[tuple(params.items())] = cluster_labels\n",
        "\n",
        "    # Convert results to a DataFrame and sort by best silhouette score\n",
        "    results_df = pd.DataFrame(results).sort_values(by='silhouette_score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n✅ Grid Search Completed! Returning results.\")\n",
        "    results_df = results_df.reset_index()\n",
        "\n",
        "    return results_df, clusters_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgAYkQsC0_RF"
      },
      "outputs": [],
      "source": [
        "results_df_kmean, clusters_dict_kmean = KMEAN_gridsearch(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_usfbvH0_RF"
      },
      "outputs": [],
      "source": [
        "results_df_kmean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJDzsL1q0_RF"
      },
      "outputs": [],
      "source": [
        "plot_data_clusters(results_df_kmean, clusters_dict_kmean, faults_df, id=0, mode='kmeans', style=\"carto-positron\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaqfPKG60_RF"
      },
      "outputs": [],
      "source": [
        "plot_2d_stat(results_df=results_df_kmean.drop(columns=['silhouette_per_cluster']), x_variable='n_clusters', y_variable='silhouette_score', color_variable='chs', title='KMEAN parameter analysis')\n",
        "plot_2d_stat(results_df=results_df_kmean.drop(columns=['silhouette_per_cluster']), x_variable='n_clusters', y_variable='chs', color_variable='silhouette_score', title='KMEAN parameter analysis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RO863Tb0_RF"
      },
      "source": [
        "## DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsOq9_tt0_RF"
      },
      "outputs": [],
      "source": [
        "def DBSCAN_gridsearch(df):\n",
        "    # Define parameter grid\n",
        "    param_grid = {\n",
        "        'eps': np.round(np.linspace(0.05, 2.15, num=int((2.15 - 0.05) / 0.05) + 1), 2).tolist(),  # from 0.2 to 1.0 with the step 0.1\n",
        "        'min_samples': np.linspace(15, 365, num=int((365 - 15) / 50) + 1, dtype=int).tolist(),  # from 25 to 375 with the step 50\n",
        "        'metric': ['euclidean', 'manhattan']  # Distance metrics\n",
        "    }\n",
        "\n",
        "    # Store results\n",
        "    results = []\n",
        "    clusters_dict = {}\n",
        "    clusters_dict['data'] = df[['x', 'y', 'z']]  # Store original data\n",
        "\n",
        "    # Total number of iterations\n",
        "    total_combinations = len(list(ParameterGrid(param_grid)))\n",
        "    current_iteration = 0\n",
        "\n",
        "    # Grid Search over parameters\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        current_iteration += 1\n",
        "        print(f\"Iteration {current_iteration}/{total_combinations}: Testing parameters {params}\")\n",
        "\n",
        "        # Apply DBSCAN clustering\n",
        "        dbscan = DBSCAN(\n",
        "            eps=params['eps'],\n",
        "            min_samples=params['min_samples'],\n",
        "            metric=params['metric']\n",
        "        )\n",
        "        cluster_labels = dbscan.fit_predict(df[['x', 'y', 'scaled_z']] )\n",
        "\n",
        "        # Ignore models where all points are labeled as noise (-1)\n",
        "        unique_labels = set(cluster_labels)\n",
        "        if len(unique_labels) <= 1:\n",
        "            print(\"  - Skipping due to all points being noise.\")\n",
        "            continue\n",
        "\n",
        "        # Compute clustering metrics\n",
        "        silhouette_avg = silhouette_score(df[['x', 'y', 'scaled_z']], cluster_labels) if len(unique_labels) > 1 else -1\n",
        "        dbi = davies_bouldin_score(df[['x', 'y', 'scaled_z']], cluster_labels)\n",
        "        chs = calinski_harabasz_score(df[['x', 'y', 'scaled_z']], cluster_labels)\n",
        "\n",
        "        # Compute silhouette scores for each sample\n",
        "        silhouette_values = silhouette_samples(df, cluster_labels)\n",
        "        silhouette_per_cluster = {label: [] for label in unique_labels if label != -1}  # Exclude noise (-1)\n",
        "        for label, score in zip(cluster_labels, silhouette_values):\n",
        "            if label != -1:  # Exclude noise points from per-cluster silhouette scores\n",
        "                silhouette_per_cluster[label].append(score)\n",
        "\n",
        "        print(f\"  - Number of clusters: {len(unique_labels) - 1}, Silhouette Score: {silhouette_avg:.4f}, DBI: {dbi:.4f}, CHS: {chs:.4f}\")\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'eps': params['eps'],\n",
        "            'min_samples': params['min_samples'],\n",
        "            'metric': params['metric'],\n",
        "            'n_clusters': len(unique_labels) - 1,\n",
        "            'silhouette_score': silhouette_avg,\n",
        "            'dbi': dbi,\n",
        "            'chs': chs,\n",
        "            'silhouette_per_cluster': silhouette_per_cluster\n",
        "        })\n",
        "\n",
        "        # Store cluster labels in dictionary\n",
        "        clusters_dict[tuple(params.items())] = cluster_labels\n",
        "\n",
        "    # Convert results to a DataFrame and sort by best silhouette score\n",
        "    results_df = pd.DataFrame(results).sort_values(by='silhouette_score', ascending=False).reset_index(drop=True)\n",
        "    results_df = results_df.reset_index()\n",
        "\n",
        "    print(\"\\n✅ Grid Search Completed! Returning results.\")\n",
        "\n",
        "    return results_df, clusters_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxabDIMK0_RG"
      },
      "outputs": [],
      "source": [
        "results_df_dbscan, clusters_dict_dbscan = DBSCAN_gridsearch(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcB9BsL_0_RG"
      },
      "outputs": [],
      "source": [
        "results_df_dbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrkwirBZ0_RG"
      },
      "outputs": [],
      "source": [
        "plot_2d_stat(results_df=results_df_dbscan.drop(columns=['silhouette_per_cluster']), x_variable='n_clusters', y_variable='silhouette_score', color_variable='eps', title='DBSCAN parameter analysis')\n",
        "plot_2d_stat(results_df=results_df_dbscan.drop(columns=['silhouette_per_cluster']), x_variable='n_clusters', y_variable='chs', color_variable='silhouette_score', title='DBSCAN parameter analysis')\n",
        "plot_2d_stat(results_df=results_df_dbscan.drop(columns=['silhouette_per_cluster']), x_variable='n_clusters', y_variable='dbi', color_variable='silhouette_score', title='DBSCAN parameter analysis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSNhmagJ0_RG"
      },
      "outputs": [],
      "source": [
        "plot_data_clusters(results_df_dbscan, clusters_dict_dbscan, faults_df, id=370, mode='dbscan', style=\"carto-positron\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "468g9z_L0_RG"
      },
      "outputs": [],
      "source": [
        "silhouette_score_level = 0.0\n",
        "num_clusters = results_df_dbscan[results_df_dbscan['silhouette_score']> silhouette_score_level]['n_clusters'].unique()\n",
        "for num_cluster in num_clusters:\n",
        "  n = results_df_dbscan[(results_df_dbscan['n_clusters']==num_cluster) & (results_df_dbscan['silhouette_score']> silhouette_score_level)]['index'].nunique()\n",
        "  print(f\"{n} clusters with {num_cluster} number of clusters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqGDfdJa0_RG"
      },
      "outputs": [],
      "source": [
        "calculate_ari_all(method='dbscan', results_df=results_df_dbscan, clusters_dict=clusters_dict_dbscan, silhouette_score_level = 0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x0EgZZW0_RG"
      },
      "source": [
        "## HDBSCAN algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9qt-vkg0_RG"
      },
      "outputs": [],
      "source": [
        "def HDBSCAN_gridsearch(df):\n",
        "    # Define parameter grid\n",
        "    param_grid = {\n",
        "        'min_cluster_size': np.linspace(20, 301, num=int((300 - 20) / 20), dtype=int).tolist(),\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    }\n",
        "\n",
        "    # Store results\n",
        "    results = []\n",
        "    clusters_dict = {}\n",
        "    clusters_dict['data'] = df[['x', 'y', 'z']]  # Store original data\n",
        "\n",
        "    # Total number of iterations\n",
        "    total_combinations = len(list(ParameterGrid(param_grid)))\n",
        "    current_iteration = 0\n",
        "\n",
        "    # Grid Search over parameters\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        current_iteration += 1\n",
        "        print(f\"Iteration {current_iteration}/{total_combinations}: Testing parameters {params}\")\n",
        "\n",
        "        # Apply HDBSCAN clustering\n",
        "        hdb = hdbscan.HDBSCAN(\n",
        "            min_cluster_size=params['min_cluster_size'],\n",
        "            metric=params['metric']\n",
        "        )\n",
        "        cluster_labels = hdb.fit_predict(df[['x', 'y', 'scaled_z']])\n",
        "\n",
        "        # Ignore models where all points are labeled as noise (-1)\n",
        "        unique_labels = set(cluster_labels)\n",
        "        if len(unique_labels) <= 1:\n",
        "            print(\"  - Skipping due to all points being noise.\")\n",
        "            continue\n",
        "\n",
        "        # Compute clustering metrics\n",
        "        silhouette_avg = silhouette_score(df[['x', 'y', 'scaled_z']], cluster_labels) if len(unique_labels) > 1 else -1\n",
        "        dbi = davies_bouldin_score(df[['x', 'y', 'scaled_z']], cluster_labels)\n",
        "        chs = calinski_harabasz_score(df[['x', 'y', 'scaled_z']], cluster_labels)\n",
        "\n",
        "        # Compute silhouette scores for each sample\n",
        "        silhouette_values = silhouette_samples(df[['x', 'y', 'scaled_z']], cluster_labels)\n",
        "        silhouette_per_cluster = {label: [] for label in unique_labels if label != -1}  # Exclude noise (-1)\n",
        "        for label, score in zip(cluster_labels, silhouette_values):\n",
        "            if label != -1:  # Exclude noise points from per-cluster silhouette scores\n",
        "                silhouette_per_cluster[label].append(score)\n",
        "\n",
        "        print(f\"  - Number of clusters {len(unique_labels) - 1}, Silhouette Score: {silhouette_avg:.4f}, DBI: {dbi:.4f}, CHS: {chs:.4f}\")\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'min_cluster_size': params['min_cluster_size'],\n",
        "            'metric': params['metric'],\n",
        "            'n_clusters': len(unique_labels) - 1,\n",
        "            'silhouette_score': silhouette_avg,\n",
        "            'dbi': dbi,\n",
        "            'chs': chs,\n",
        "            'silhouette_per_cluster': silhouette_per_cluster\n",
        "        })\n",
        "\n",
        "        # Store cluster labels in dictionary\n",
        "        clusters_dict[tuple(params.items())] = cluster_labels\n",
        "\n",
        "    # Convert results to a DataFrame and sort by best silhouette score\n",
        "    results_df = pd.DataFrame(results).sort_values(by='silhouette_score', ascending=False).reset_index(drop=True)\n",
        "    results_df = results_df.reset_index()\n",
        "    print(\"\\n✅ Grid Search Completed! Returning results.\")\n",
        "\n",
        "    return results_df, clusters_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHk8EROc0_RH"
      },
      "outputs": [],
      "source": [
        "results_df_hdbscan, clusters_dict_hdbscan = HDBSCAN_gridsearch(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smYUoONA0_RH"
      },
      "outputs": [],
      "source": [
        "results_df_hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVzaRznF0_RH"
      },
      "outputs": [],
      "source": [
        "plot_2d_stat(results_df=results_df_hdbscan.drop(columns=['silhouette_per_cluster']), x_variable='n_clusters', y_variable='silhouette_score', color_variable='min_cluster_size', title='HDBSCAN parameter analysis')\n",
        "plot_2d_stat(results_df=results_df_hdbscan.drop(columns=['silhouette_per_cluster']), x_variable='n_clusters', y_variable='chs', color_variable='min_cluster_size', title='HDBSCAN parameter analysis')\n",
        "plot_2d_stat(results_df=results_df_hdbscan.drop(columns=['silhouette_per_cluster']), x_variable='n_clusters', y_variable='dbi', color_variable='min_cluster_size', title='HDBSCAN parameter analysis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MZOtiH30_RH"
      },
      "outputs": [],
      "source": [
        "plot_data_clusters(results_df_hdbscan, clusters_dict_hdbscan, faults_df, id=0, mode='hdbscan', style=\"carto-positron\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcU1Eskt0_RH"
      },
      "outputs": [],
      "source": [
        "silhouette_score_level = 0.1\n",
        "num_clusters = results_df_hdbscan[results_df_hdbscan['silhouette_score']> silhouette_score_level]['n_clusters'].unique()\n",
        "for num_cluster in num_clusters:\n",
        "  n = results_df_hdbscan[(results_df_hdbscan['n_clusters']==num_cluster) & (results_df_hdbscan['silhouette_score']> silhouette_score_level)]['index'].nunique()\n",
        "  print(f\"{n} clusters with {num_cluster} number of clusters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfzGoo4J0_RH"
      },
      "outputs": [],
      "source": [
        "calculate_ari_all('hdbscan',results_df_hdbscan, clusters_dict_hdbscan, silhouette_score_level = 0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYAFS69s0_RH"
      },
      "source": [
        "# DOWNLOAD RESULTED DATA AS TXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfiyV8na0_RH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def save_cluster_result(method, results_df, cluster_dic, id):\n",
        "    param_key = get_param_key(method, results_df, id)\n",
        "\n",
        "    data_result = cluster_dic['data'].copy()\n",
        "    data_result['cluster'] = cluster_dic[param_key]\n",
        "\n",
        "    # Define and create the output folder\n",
        "    folder_name = f'Cluster_result_{method}_id_{id}'\n",
        "    os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "    for cluster_label in data_result['cluster'].unique():\n",
        "        # Filter the DataFrame for the current cluster\n",
        "        cluster_data = data_result[data_result['cluster'] == cluster_label][['y', 'x', 'z']]\n",
        "\n",
        "        # Define the full file path\n",
        "        filename = f'Cluster_result_{method}_id_{id}_cluster_{cluster_label}.dat'\n",
        "        file_path = os.path.join(folder_name, filename)\n",
        "\n",
        "        # Save the filtered data to a .dat file\n",
        "        cluster_data.to_csv(file_path, sep=' ', index=False, header=False)\n",
        "\n",
        "        print(f'Saved {filename} to {folder_name}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKJk64ax0_RI"
      },
      "outputs": [],
      "source": [
        "save_cluster_result('hdbscan', results_df_hdbscan, clusters_dict_hdbscan, id=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "earth_analysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}